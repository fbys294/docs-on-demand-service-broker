---
title: Operating an On-Demand Broker
owner: London Services Enablement
---

## <a id="operator"></a>Operator Responsibilities
The operator is responsible for performing the following:

* Request appropriate networking rules for on-demand service tiles.
* Configure the BOSH Director
* Upload the required releases for the broker deployment and service instance deployments.
* Write a broker manifest
  * If you are unfamiliar with writing BOSH v2 manifests, see [Manifest v2 Schema](http://bosh.io/docs/manifest-v2.html).
  * Core broker configuration
  * Service catalog and plan composition
* Manage brokers
* Documentation for the operator

For a list of deliverables provided by the Service Author, see [Required Deliverables](creating.html#what-is-required-of-the-service-authors).

For an example manifest for a Redis service, see [redis-example-service-adapter-release](https://github.com/pivotal-cf-experimental/redis-example-service-adapter-release/blob/master/docs/example-manifest.yml).

For an example manifest for a Kafka service, see [kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml).

## <a id="about-cli"></a>About the BOSH CLI
The BOSH CLI is available in two major versions, v1 and v2. Pivotal recommends that you use the BOSH CLI v2 when possible.

This topic provides examples of using each version of the BOSH CLI. Your PCF installation may affect which version of the BOSH CLI you can use. Consult the table below to determine which version of the CLI is supported for your installation.

<table class="nice">
    <th>PCF Version</th>
    <th>BOSH CLI Version</th>
    <tr>
        <td>1.10</td>
        <td>CLI v1</td>
    </tr>
    <tr>
        <td>1.11</td>
        <td>CLI v1 or CLI v2 (Pivotal recommends CLI v2)</td>
    </tr>
    <tr>
        <td>1.12 and later</td>
        <td>CLI v2</td>
    </tr>
</table>

## <a id="networking"></a>Set Up Networking

<%= partial 'service_networks_table' %>

<br>
Regardless of the specific network layout, the operator must ensure network 
rules are set up so that connections are open as described in the table below. 

<table class="nice">
  <th>This component...</th>
  <th>Must communicate with...</th>
  <th>Default TCP Port</th>
  <th>Communication direction(s)</th>
  <th>Notes</th>
  <tr>
    <td><strong>ODB</strong></td>
    <td>
        <ul>
            <li><strong>BOSH Director</strong></li>
          <li><strong>BOSH UAA</strong></li>
        </ul>
    </td>
    <td>
      <ul>
        <li>25555</li>
        <li>8443</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default ports are not configurable.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service (such as RabbitMQ for PCF). 
      May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for administrative tasks. 
      Avoid opening general use, app-specific ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>ODB</strong></td>
    <td><strong>PAS (or Elastic Runtime)</strong>
    </td>
    <td>8443</td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>Errand VMs</strong></td>
    <td>
      <ul>
        <li><strong>PAS (or Elastic Runtime) </strong></li>
        <li><strong>ODB</strong></li>
        <li><strong>Deployed Service Instances</strong></li>
      </ul>
    </td>
    <td>
      <ul>
        <li>8443</li>
        <li>8080</li>
        <li>Specific to the service. May be one or more ports.</li>
      </ul>
    </td>
    <td>One-way</td>
    <td>The default port is not configurable.</td>
  </tr>
  <tr>
    <td><strong>BOSH Agent</strong></td>
    <td><strong>BOSH Director</strong>
    </td>
    <td>4222</td>
    <td>Two-way</td>
    <td>The BOSH Agent runs on every VM in the system, including the BOSH Director VM. 
      The BOSH Agent initiates the connection with the BOSH Director.<br>
      The default port is not configurable.  </td>
  </tr>
  <tr>
    <td><strong>Deployed apps on PAS (or Elastic Runtime)</strong></td>
    <td><strong>Deployed service instances</strong>
    </td>
    <td>Specific to the service. May be one or more ports.</td>
    <td>One-way</td>
    <td>This connection is for general use, app-specific tasks. 
      Avoid opening administrative ports for this connection.</td>
  </tr>
  <tr>
    <td><strong>PAS (or Elastic Runtime)</strong></td>
    <td><strong>ODB</strong>
    </td>
    <td>8080</td>
    <td>One-way</td>
    <td>This port may be different for individual services. 
      This port may also be configurable by the operator if allowed by the 
      tile developer.</td>
  </tr>
</table>

## <a id="configure-bosh"></a>Set Up Your BOSH Director
Dependencies for the On-Demand Broker:

- BOSH Director v257 or later (PCF 1.8) **Note:** does not support BOSH Windows
- Cloud Foundry v238 or later (PCF 1.8)

 <p class="note"><strong>Note</strong>: [Service instance lifecycle errands](#lifecycle-errands) require BOSH Director v261 (PCF 1.10) or later.</p>

### <a id="ssl-certs"></a>SSL Certificates

If ODB is configured to communicate with BOSH on the Director's public IP you may be using a self-signed certificate unless you have a domain for your BOSH Director. ODB does not ignore TLS certificate validation errors by default (as expected). You have two options to configure certificate-based authentication between the BOSH Director and the ODB:

1. Add the BOSH Director's root certificate to ODB's trusted pool in the ODB manifest:

    ```yaml
    bosh:
      root_ca_cert: ROOT-CA-CERT
    ```

1. Use BOSH's `trusted_certs` feature to add a self-signed CA certificate to each VM BOSH deploys. For more details on how to generate and use self-signed certificates for BOSH Director and UAA, see [Director SSL Certificate Configuration](https://bosh.io/docs/director-certs.html).

You can also configure a separate root CA certificate that is used when ODB communicates with the Cloud Foundry API (Cloud Controller).
This is done in a similar way to above.
For more information, see [manifest snippets below](#core-broker-configuration).

### <a id="bosh-teams"></a>BOSH Teams

BOSH has a teams feature that allows you to further control how BOSH operations are available to different clients. We strongly recommend using it to ensure that your on-demand service broker client can only modify deployments it created. For example, if you [use uaac](https://docs.cloudfoundry.org/adminguide/uaa-user-management.html) to create the client like this:

```bash
uaac client add CLIENT-ID \
  --secret CLIENT-SECRET \
  --authorized_grant_types "refresh_token password client_credentials" \
  --authorities "bosh.teams.TEAM-NAME.admin"
```

Then when you [configure the broker's BOSH authentication](#core-broker-configuration), you can use this client ID and secret. The broker will then only be able to perform BOSH operations on deployments it has created itself.

For how to set up and use BOSH teams, see [Director teams and permissions configuration](https://bosh.io/docs/director-users-uaa-perms.html).

For more details on securing how ODB uses BOSH, see [Security](#security).

### <a id="cloud-controller"></a>Cloud Controller

ODB used the Cloud Controller as a source of truth about service offerings, plans, and instances. To reach Cloud Controller, ODB needs to be configured with credentials. These can be either client or user credentials:

* Client credentials: as of Cloud Foundry v238, the UAA client must have authority `cloud_controller.admin`.
* User credentials: a Cloud Foundry admin user, i.e. a member of the `scim.read` and `cloud_controller.admin` groups as a minimum.

Detailed broker configuration is covered [below](#core-broker-configuration).

## <a id="upload-releases"></a>Upload Required Releases

Upload the following releases to the BOSH Director:

* on-demand-service-broker
* your service adapter
* your service release(s)

## <a id="broker-manifest"></a>Write a Broker Manifest

### <a id="core-broker-config"></a>Core Broker Configuration

Your manifest should contain one non-errand instance group, that co-locates both:

* the `broker` job from on-demand-service-broker
* your service adapter job from your service adapter release

The broker is stateless and does not need a persistent disk. The VM type can be quite small: a single CPU and 1 GB of memory should be sufficient in most cases.

An example snippet is shown below:

```yaml
instance_groups:
  - name: broker # this can be anything
    instances: 1
    vm_type: VM-TYPE
    stemcell: STEMCELL
    networks:
      - name: NETWORK
    jobs:
      - name: SERVICE-ADAPTER-JOB-NAME
        release: SERVICE-ADAPTER-RELEASE
      - name: broker
        release: on-demand-service-broker
        properties:
          # choose a port and basic auth credentials for the broker
          port: BROKER-PORT
          username: BROKER-USERNAME
          password: BROKER-PASSWORD
          disable_ssl_cert_verification: true|false # optional, defaults to false. This should NOT set to true in production
          shutdown_timeout_in_seconds: 60 # optional, defaults to 60 seconds. This allows the broker to gracefully wait for any open requests to complete before shutting down.
          expose_operational_errors: true|false # optional, defaults to false. This allows for BOSH operational errors to be displayed for the CF user.
          enable_plan_schemas: true|false # optional, defaults to false. If set to true, plan schemas are included in the catalog, and the broker fails if the adapter does not implement generate-plan-schemas.
          cf:
            url: CF-API-URL
            root_ca_cert: CA-CERT-FOR-CLOUD-CONTROLLER # optional, see SSL certificates
            authentication: # either client_credentials or user_credentials, not both as shown
              url: CF-UAA-URL
              client_credentials:
                client_id: UAA-CLIENT-ID # with cloud_controller.admin authority and client_credentials in the authorized_grant_type
                secret: UAA-CLIENT-SECRET
              user_credentials:
                username: CF-ADMIN-USERNAME # in the cloud_controller.admin and scim.read groups
                password: CF-ADMIN-PASSWORD
          bosh:
            url: DIRECTOR-URL
            root_ca_cert: CA-CERT-FOR-BOSH-DIRECTOR-AND-ASSOCIATED-UAA # optional, see SSL certificates
            authentication: # either basic or uaa, not both as shown
              basic:
                username: BOSH-USERNAME
                password: BOSH-PASSWORD
              uaa:
                client_id: BOSH-CLIENT-ID
                client_secret: BOSH-CLIENT-SECRET
          service_adapter:
            path: PATH-TO-SERVICE-ADAPTER-BINARY # optional, provided by the Service Author. Defaults to /var/vcap/packages/odb-service-adapter/bin/service-adapter

          # There are more broker properties that are discussed below
```

<p class="note"><strong>Note</strong>: The <code>disable_ssl_cert_verification</code> 
  option is dangerous and <strong>should not be set to true in production</strong>.</p>

This snippet is using the BOSH v2 syntax and making use of global cloud config and job-level properties.

### <a id="catalog"></a>Service Catalog and Plan Composition

The operator must:

1. Supply each release job specified by the Service Author exactly once. You can include releases that provide many jobs, as long as each required job is provided by exactly one release.
1. Supply one stemcell that is used on each VM in the service deployments. ODB does not currently support service instance deployments that use a different stemcell for different instance groups.
1. Use exact versions for releases and stemcells. The use of `latest` and floating stemcells are not supported.
1. Create Cloud Foundry [service metadata](https://docs.pivotal.io/pivotalcf/services/catalog-metadata.html#services-metadata-fields) in the catalog for the service offering.
   This metadata will be aggregated in the Cloud Foundry marketplace and displayed in Apps Manager and the `cf` CLI.
1. Compose plans. In ODB, service authors do not define plans but instead expose plan properties. The operator's role is to compose combinations of these properties, along with IAAS resources and catalog metadata into as many plans as they like.
  1. Create Cloud Foundry [service plan metadata](https://docs.pivotal.io/pivotalcf/services/catalog-metadata.html#plan-metadata-fields) in the service catalog for each plan.
  1. Provide resource mapping for each instance group specified by the Service Author for each plan.
     The resource values must correspond to valid resource definitions in the BOSH Director's global cloud config.
     In some cases Service Authors will recommend resource configuration: e.g. in single-node Redis deployments, an instance count greater than 1 does not make sense.
     Here the operator can configure the deployment to span multiple availability zones, by using the [BOSH multi-az feature](https://bosh.io/docs/azs.html). For example the [kafka multi az plan](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml#L100). In some cases, service authors will provide errands for the service release. You can add an instance group of type errand by setting the lifecycle field. For example the [smoke_tests for the kafka deployment](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml#L93).
  1. Provide values for plan properties.
     Plan properties are key-value pairs defined by the Service Author. Some examples include a boolean to enable disk persistence for Redis, and a list of strings representing RabbitMQ plugins to load. The Service Author should document whether these properties are mandatory or optional, whether the use of one property precludes the use of another, and whether certain properties affect recommended instance group to resource mappings.
     Properties can also be specified at the service offering level, where they will be applied to every plan. If there is a conflict between global and plan-level properties, the plan properties will take precedence.
  1. Provide an (optional) update block for each plan.
     You may require plan-specific configuration for BOSH's update instance operation. The ODB passes the plan-specific update block to the service adapter. Plan-specific update blocks should have the same structure as [the update block in a BOSH manifest](https://bosh.io/docs/manifest-v2.html#update). The Service Author can define a default update block to be used when a plan-specific update block is not provided, and whether the service adapter supports their configuration in the manifest.

Add the snippet below to your broker job properties section:

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION # Exact release version
      jobs: [RELEASE-JOBS-NEEDED-FOR-DEPLOYMENT-AND-LIFECYCLE-ERRANDS] # Service Author will specify list of jobs required
  stemcell: # every instance group in the service deployment has the same stemcell
    os: SERVICE-STEMCELL
    version: SERVICE-STEMCELL-VERSION # Exact stemcell version
service_catalog:
  id: CF-MARKETPLACE-ID
  service_name: CF-MARKETPLACE-SERVICE-OFFERING-NAME
  service_description: CF-MARKETPLACE-DESCRIPTION
  bindable: TRUE|FALSE
  plan_updatable: TRUE|FALSE # optional
  tags: [TAGS] # optional
  requires: [REQUIRED-PERMISSIONS] # optional
  dashboard_client: # optional
    id: DASHBOARD-OAUTH-CLIENT-ID
    secret: DASHBOARD-OAUTH-CLIENT-SECRET
    redirect_uri: DASHBOARD-OAUTH-REDIRECT-URI
  metadata: # optional
    display_name: DISPLAY-NAME
    image_url: IMAGE-URL
    long_description: LONG-DESCRIPTION
    provider_display_name: PROVIDER-DISPLAY-NAME
    documentation_url: DOCUMENTATION-URL
    support_url: SUPPORT-URL
  global_properties: {} # optional, applied to every plan.
  global_quotas: # optional
    service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances across all plans
  plans:
    - name: CF-MARKETPLACE-PLAN-NAME
      plan_id: CF-MARKETPLACE-PLAN-ID
      description: CF-MARKETPLACE-DESCRIPTION
      cf_service_access: ENABLE|DISABLE|MANUAL # optional, enable by default.
      bindable: TRUE|FALSE # optional. If specified, this takes precedence over the bindable attribute of the service
      metadata: # optional
        display_name: DISPLAY-NAME
        bullets: [BULLET1, BULLET2]
        costs:
          - amount:
              CURRENCY-CODE-STRING: CURRENCY-AMOUNT-FLOAT
            unit: FREQUENCY-OF-COST
      quotas: # optional
        service_instance_limit: INSTANCE-LIMIT # the maximum number of service instances for this plan
      instance_groups: # resource mapping for the instance groups defined by the Service Author
        - name: SERVICE-AUTHOR-PROVIDED-INSTANCE-GROUP-NAME
          vm_type: VM-TYPE
          vm_extensions: [VM-EXTENSIONS] # optional
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
          persistent_disk_type: DISK # optional
        - name: SERVICE-AUTHOR-PROVIDED-LIFECYCLE-ERRAND-NAME # optional
          lifecycle: errand
          vm_type: VM-TYPE
          instances: INSTANCE-COUNT
          networks: [NETWORK]
          azs: [AZ]
      properties: {} # valid property key-value pairs are defined by the Service Author
      update: # optional
        canaries: 1 # optional
        max_in_flight: 2  # required
        canary_watch_time: 1000-30000 # required
        update_watch_time: 1000-30000 # required
        serial: true # optional
      lifecycle_errands: #optional
        post_deploy:
          name: ERRAND-NAME #optional
          instances: [INSTANCE-NAME] #optional, for co-locating errand
        pre_delete: ERRAND-NAME #optional
```

## <a id="secure-binding"></a>Enabling Secure Binding

ODB optionally allows secure storage of service credentials in CredHub. To use
this feature, you will need to be able to connect to CredHub 1.6.X or later.
This may be provided as part of your Cloud Foundry deployment.
Credhub depends on being addressed by a local domain name so your instance group
will require access to the local DNS provider. Most Cloud Foundry deployments
currently use Consul as a DNS provider. In the ODB manifest, you will need to consume
the credhub link and include the secure binding credentials section.
You may also wish to consume Consul links to enable DNS.
An example manifest snippet assuming you are connecting to a Cloud Foundry deployment
is shown below:

```
instance_groups:
- name: broker
  ...
  jobs:
  - name: broker
    consumes:
      credhub:
        from: credhub
        deployment: cf
    properties:
      ...
      secure_binding_credentials:
        enabled: true
        authentication:
          uaa:
            client_id: ((credhub.client_id))
            client_secret: ((credhub.client_secret))
            ca_cert: ((cf.uaa.ca_cert))
  - name: consul_agent
    release: consul
    consumes:
      consul: nil
      consul_client:
        from: consul_client_link
        deployment: cf
      consul_common:
        from: consul_common_link
        deployment: cf
      consul_server: nil
```

<p class="note"><strong>Note</strong>: The CredHub UAA Client must have
<code>credhub.write</code> and <code>credhub.read</code> in its list of <code>authorities</code>.</p>

If the Secure Binding Credentials section is omitted, or if
`secure_binding_credentials.enabled` is `false`, the service credentials will be
set explicitly in the application's VCAP_SERVICES environment variable.

### <a id="credhub-key-format"></a> How credentials are stored on CredHub

The credentials for a given Service Binding are stored with the following format:

```
/C/:SERVICE-GUID/:SERVICE-INSTANCE-GUID/:BINDING-GUID/CREDENTIALS
```

The plain-text credentials are stored in CredHub under this key, and the key is
available under the VCAP_SERVICES environment variable for the application.

## <a id="route"></a>Route Registration

You can optionally colocate the `route_registrar` job from the [routing release](http://bosh.io/releases/github.com/cloudfoundry-incubator/cf-routing-release?all=1) with the on-demand-service-broker, in order to:

1. load balance multiple instances of ODB using Cloud Foundry's router
1. access ODB from the public internet

To do this, upload the release to your BOSH Director and [configure the job properties](http://bosh.io/jobs/route_registrar?source=github.com/cloudfoundry-incubator/cf-routing-release&version=0.136.0), replacing the version in that docs URL query string as appropriate.

Remember to set the `broker_uri` property in the [register-broker errand](#register-broker) if you configure a route.

## <a id="service-instance-quotas"></a>Service Instance Quotas

ODB offers global and plan level service quotas to set service instance limits.

Plan quotas restrict the number of service instances for a given plan, while
the global limit restricts the number of service instances across all plans.

When creating a service instance, ODB will check the global service instance limit.
If it has not been reached, it will check the plan service instance limit.

<p class="note"><strong>Note</strong>: These limits do not include orphans. See <a href="#listing-orphans">List Orphan Deployments</a>
   and <a href="#orphan-deployments">Delete Orphaned Deployments</a>.</p>

## <a id="broker-metrics"></a>Broker Metrics

The ODB BOSH release contains a metrics job, that can be used to emit metrics when colocated with [service metrics](https://network.pivotal.io/products/service-metrics-sdk/). You must include the [loggregator release](https://github.com/cloudfoundry/loggregator) in order to do this.

Add the following jobs to the broker instance group:

```yaml
- name: service-metrics
  release: service-metrics
  properties:
    service_metrics:
      execution_interval_seconds: INTERVAL-BETWEEN-SUCCESSIVE-METRICS-COLLECTIONS
      origin: ORIGIN-TAG-FOR-METRICS
      monit_dependencies: [broker] # hardcode this
      ....snip....
      #Add Loggregator configuration here: see examples @ https://github.com/pivotal-cf/service-metrics-release/blob/master/manifests
      ....snip....
- name: service-metrics-adapter
  release: ODB-RELEASE
```

An example of how the service metrics can be configured for an on-demand-broker deployment can be seen in the [kafka-example-service-adapter-release](https://github.com/pivotal-cf-experimental/kafka-example-service-adapter-release/blob/master/docs/example-manifest.yml#L106) manifest.

Pivotal have tested this example configuration with loggregator v58 and service-metrics v1.5.0.

For more information about service metrics, see [Service Metrics for Pivotal Cloud Foundry](http://docs.pivotal.io/service-metrics).

## <a id="startup-checks"></a>Broker Startup Checks

The following startup checks occur:

* Verify that the CF and BOSH versions satisfy the minimum requirement. **Note**: If your service offering includes lifecycle errands, the minimum requirement for BOSH will be higher. See [Set Up Your BOSH Director](#configure-bosh). If your system does not meet minimum requirements, you will see an insufficient version error. For example:
```
CF API error: Cloud Foundry API version is insufficient, ODB requires CF v238+.
```
* Verify that, for the service offering, no plan IDs have changed for plans that have existing service instances. If there are instances, you will see following error:
```
You cannot change the plan_id of a plan that has existing service instances.
```

## <a id="broker-stop"></a>Broker Shutdown

The broker will try and wait for any incomplete https requests to complete before shutting down. This allows us to reduce the risk of leaving orphan deployments in the event that the BOSH Director does not respond to our initial `bosh deploy` request. The `broker.shutdown_timeout` property determines how long the broker will wait before it is forced to shutdown. The default is 60 seconds, but can be configured in the manifest. Please refer to how to <a href="#broker-manifest">Write a Broker Manifest</a>

## <a id="lifecycle-errands"></a>Service Instance Lifecycle Errands

 <p class="note"><strong>Note</strong>: This feature requires BOSH Director v261 or later.</p>

Service Instance lifecycle errands allow additional short lived jobs to be run as part of service instance deployment. A deployment is only considered successful if the lifecycle errands successfully exit.

The service adapter must offer this errand as part of the service instance deployment.

ODB supports the following lifecycle errands:

- `post_deploy` - Run after the creation or updating of a service instance. An example use case is running a health check to ensure the service instance is functioning. See the workflow [here](./concepts.html#post-deploy)
- `pre_delete` - Run before the deletion of a service instance. An example use case is cleaning up data prior to a service shutdown. See the workflow [here](./concepts.html#pre-delete)

Service Instance lifecycle errands are configured on a per-plan basis. To enable lifecycle errands, the errand job must be:

- Added to the service instance deployment.
- Added to the plan's instance groups.
- Set in the plan's lifecycle errands configuration.

An example manifest snippet configuring lifecycle errands for a plan:

<p class="note warning">The property <code>lifecycle_errands.pre_delete</code> must be specified as an array as of v0.21 of ODB.
This enables configuration of multiple lifecycle errands.</p>

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION
      jobs:
      - SERVICE-RELEASE-JOB
      - POST-DEPLOY-ERRAND-JOB
      - PRE-DELETE-ERRAND-JOB
service_catalog:
  plans:
      - name: CF-MARKETPLACE-PLAN-NAME
        lifecycle_errands:
          post_deploy:
            name: POST-DEPLOY-ERRAND-JOB
          pre_delete:
          - name: PRE-DELETE-ERRAND-JOB
        instance_groups:
          - name: SERVICE-RELEASE-JOB
            ...
          - name: POST-DEPLOY-ERRAND-JOB
            lifecycle: errand
            vm_type: VM-TYPE
            instances: INSTANCE-COUNT
            networks: [NETWORK]
            azs: [AZ]
          - name: PRE-DELETE-ERRAND-JOB
            lifecycle: errand
            vm_type: VM-TYPE
            instances: INSTANCE-COUNT
            networks: [NETWORK]
            azs: [AZ]
```

Changing a plan's lifecycle errands configuration while an existing deployment is in progress is not supported.
Lifecycle errands will not be run.

#### Service Instance Lifecycle Colocated Errands

<p class="note"><strong>Note</strong>: This feature requires BOSH Director v263 or later.</p>

A job errand can be run as a `post-deploy` or `pre-delete` colocated errand. Colocated errands run on an existing service instance group, avoiding additional resource allocation. For more information see the [BOSH documentation](https://bosh.io/docs/errands.html).

Similarly to the other lifecycle errands, colocated errands are deployed on a per-plan basis. Currently the ODB supports only colocated post-deploy or pre-delete errands.
In order to enable a colocated errand, the errand job must be:

- Added to the service instance deployment (1).
- Set in the plan's lifecycle errands configuration (2).
- Set the instances where the errand should run on (3).

An example manifest for colocated post-deploy errand looks like the following:

```yaml
service_deployment:
  releases:
    - name: SERVICE-RELEASE
      version: SERVICE-RELEASE-VERSION
      jobs:
      - SERVICE-RELEASE-JOB
      - POST-DEPLOY-ERRAND-JOB # (1)
service_catalog:
  plans:
      - name: CF-MARKETPLACE-PLAN-NAME
        lifecycle_errands:
          post_deploy:
            name: POST-DEPLOY-ERRAND-JOB # (2)
            instances: [SERVICE-RELEASE-JOB/0] # (3)
        instance_groups:
          - name: SERVICE-RELEASE-JOB
            ...
```

## <a id="management"></a>Broker Management

Management tasks on the broker are performed with BOSH errands.

### <a id="register"></a>Register Broker

This errand registers the broker with Cloud Foundry and enables access to plans in the service catalog. The errand should be run whenever the broker is re-deployed with new catalog metadata to update the Cloud Foundry catalog.

If the `broker_uri` property is set, then you should also register a route for your broker with Cloud Foundry. See [Route registration](#route-registration) section for more details.

When `enable_service_access: false` is set, the errand will not change service access for any plan.

Individual plans can be enabled via the optional `cf_service_access` property. This property accepts three values: `enable`, `disable`, `manual`.

* `cf_service_access: enable`: register-broker errand will enable access for that plan
* `cf_service_access: disable`: register broker errand will disable access for that plan
* `cf_service_access: manual`: register-broker errand will perform no action

If the `cf_service_access` property is not set at all, the register-broker errand will enable access for that plan.

Plans with disabled service access will not be visible to non-admin Cloud Foundry users (including Org Managers and Space Managers). Admin Cloud Foundry users can see all plans including those with disabled service access.

Add the following instance group to your manifest:

```yaml
- name: register-broker
  lifecycle: errand
  instances: 1
  jobs:
    - name: register-broker
      release: ODB-RELEASE-NAME
      properties:
        broker_name: BROKER-NAME
        broker_uri: BROKER-URI # optional, only required when a route has been registered
        disable_ssl_cert_verification: TRUE|FALSE # defaults to false
        enable_service_access: TRUE|FALSE # defaults to true
        cf:
          api_url: CF-API-URL
          admin_username: CF-API-ADMIN-USERNAME
          admin_password: CF-API-ADMIN-PASSWORD
  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [NETWORK]
  azs: [AZ]
```

Run the errand with `bosh run errand register-broker`. For v2 of the BOSH CLI, use `bosh2 run-errand register-broker`.

### <a id="delete-instances-and-deregister"></a>Delete All Service Instances and Deregister Broker

This errand performs a similar operation to the errand [delete-all-service-instances](#delete-instances) and [deregister-broker](#deregister-broker).
In addition, it also disables service access to the service offering before deleting all the instances,
and then deregisters the broker after all instances have been successfully deleted.
Pivotal disables service access to ensure that new instances cannot be provisioned during the lifetime of the errand.

The errand does the following:

1. Disables service access to the service offering for all orgs and spaces
1. Unbinds all applications from the service instances
1. Deletes all service instances sequentially
1. Deregisters the broker from Cloud Foundry

**This errand should only be used with extreme caution when you want to totally destroy all of the on-demand service instances and deregister the broker from the Cloud Foundry.**

Add the following instance group to your manifest:

```yaml
- name: delete-all-service-instances-and-deregister-broker
  lifecycle: errand
  instances: 1
  jobs:
    - name: delete-all-service-instances-and-deregister-broker
      release: ODB-RELEASE-NAME
      properties:
        broker_name: BROKER-NAME
        polling_interval_seconds: INTERVAL-IN-SECONDS # defaults to 60
        polling_initial_offset_seconds: OFFSET-IN-SECONDS # defaults to 5

  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [{name: NETWORK}]
  azs: [AZ]
```

Where: <br>
`INTERVAL-IN-SECONDS`: The interval in seconds before a service instance is deleted. <br>
`OFFSET-IN-SECONDS`: The offset in seconds before polling Cloud Foundry to check 
if the instance has been deleted.

 <p class="note"><strong>Note</strong>: The `polling_interval_seconds` default is set to 60 seconds
    because the Cloud Controller itself polls the on-demand broker every 60 seconds.
    Setting your polling interval to anything lower than 60 seconds will not speed up the errand.<br/>
    The `polling_initial_offset_seconds` default is set to 5 seconds.
    In systems with more load, consider increasing the polling offset.

Run the errand with `bosh run errand delete-all-service-instances-and-deregister-broker`.

### <a id="deregister-broker"></a>Deregister Broker

This errand deregisters a broker from Cloud Foundry. It requires that there are no existing service instances.

Add the following instance group to your manifest:

```yaml
- name: deregister-broker
  lifecycle: errand
  instances: 1
  jobs:
    - name: deregister-broker
      release: ODB-RELEASE-NAME
      properties:
        broker_name: BROKER-NAME
  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [{name: SERVICE-NETWORK}]
  azs: [AZ]
```

Run the errand with `bosh run errand deregister-broker`. For v2 of the BOSH CLI, use `bosh2 run-errand deregister-broker`.

### <a id="delete-instances"></a>Delete All Service Instances

This errand deletes all service instances of your broker's service offering in every org and space of Cloud Foundry.
It uses the Cloud Controller API to do this, and therefore only deletes instances the Cloud Controller knows about.
It does not delete orphan BOSH deployments: those that don't correspond to a known service instance.
Orphan BOSH deployments *should* never happen, but in practice they might.
Use the [orphan-deployments errand](#listing-orphans) to identify them.

The errand does the following:

1. Unbinds all applications from the service instances
1. Deletes all service instances sequentially
1. Checks if any instances have been created while the errand was running
1. If instances are detected the errand fails
1. Re-runs the errand

**This errand should only be used with extreme caution when you want to totally destroy all of the on-demand service instances from Cloud Foundry.**

Add the following instance group to your manifest:

```yaml
- name: delete-all-service-instances
  lifecycle: errand
  instances: 1
  jobs:
    - name: delete-all-service-instances
      release: ODB-RELEASE-NAME
      properties:
        polling_interval_seconds: INTERVAL-IN-SECONDS # defaults to 60
        polling_initial_offset_seconds: OFFSET-IN-SECONDS # defaults to 5

  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [{name: NETWORK}]
  azs: [AZ]
```

Where:<br>
`INTERVAL-IN-SECONDS`: The interval in seconds before a service instance is deleted. <br>
`OFFSET-IN-SECONDS`: The offset in seconds before polling Cloud Foundry to check 
if the instance has been deleted.

 <p class="note"><strong>Note</strong>: The `polling_interval_seconds` default is set to 60 seconds
    because the Cloud Controller itself polls the on-demand broker every 60 seconds.
    Setting your polling interval to anything lower than 60 seconds will not speed up the errand.<br/>
    The `polling_initial_offset_seconds` default is set to 5 seconds
    because we don't want the delete all errand to start polling cloudfoundry before
    cloudfoundry has finished processing the delete request and has contacted the broker.
    In systems with more load on cloudfoundry, this process could take a longer,
    in which case you might consider increasing the polling offset.</p>

Run the errand with `bosh run errand delete-all-service-instances`.

### <a id="orphan-deployments"></a>Delete Orphaned Deployments

The deployment for a service instance is defined as 'Orphaned' when the BOSH
deployment is still running, but the service is no longer registered in
Cloud Foundry.

The `orphan-deployments` errand will collate a list of service deployments that
have no matching service instances in Cloud Foundry and return the list to the
operator. It is then up to the operator to remove the orphaned BOSH deployments.

Add the following instance group to your manifest:

```yaml
- name: orphan-deployments
  lifecycle: errand
  instances: 1
  jobs:
  - name: orphan-deployments
    release: ODB-RELEASE-NAME
  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [{name: NETWORK}]
  azs: [AZ]
```

Run the errand with `bosh run errand orphan-deployments`. For v2 of the BOSH CLI, use `bosh2 run-errand orphan-deployments`.

If orphan deployments are present, the errand will output a list of deployment
names

```shell
[stdout]
[{"deployment_name":"service-instance_aoeu39fgn-8125-05h2-9023-9vbxf7676f3"}]

[stderr]
None

Errand 'orphan-deployments' completed successfully (exit code 0)
```

**Warning:** Deleting the bosh deployment will destroy the vm, any data present will be lost.

To delete the orphan deployment run `bosh delete deployment service-instance_aoeu39fgn-8125-05h2-9023-9vbxf7676f3`.

## <a id="updates"></a>Updates

### <a id="update-broker"></a>Update Broker

To update the [core broker configuration](#core-broker-config):

* make any necessary changes to the core broker configuration in the broker manifest
* deploy the broker

### <a id="updating-service-offering"></a>Update Service Offering

To update the service offering:

* make any changes to properties in the `service_catalog` of the broker manifest. For example, update the service metadata.
* make any changes to properties in the `service_deployment` of the broker manifest. For example, update the jobs used from a service release.
* deploy the broker

**Warning:** Once the broker has been registered with Cloud Foundry do not change the `service_id` or the `plan_id` for any plan. When the ODB starts it checks that all existing service instances in Cloud Foundry have a `plan_id` that exists in the `service_catalog`.

After changing the `service_catalog`, you should run the [register-broker errand](#register) to update the Cloud Foundry marketplace.

When the plans are updated in the `service_catalog`, then upgrades will need to be applied to existing service instances. See [upgrading all service instances](#upgrade-all-instances).

### <a id="disabling-service-plans"></a>Disable Service Plans

Access to a service plan can be disabled by using the Cloud Foundry CLI:
```
cf disable-service-access SERVICE-NAME-FROM-CATALOG -p PLAN-NAME
```

Also, when a plan has the property `cf_service_access: disable` in the `service_catalog` then the [register-broker errand](#register) errand will disable service access to that plan.

### <a id="remove-plans"></a>Remove Service Plans

A service plans can be removed if there are no instances using the plan. To remove the a plan, remove it from the broker manifest and update the Cloud Foundry marketplace by running the [register-broker errand](#register).

**Warning:** If any service instances remain on a plan that has been removed from the catalog then the ODB will fail to start.

## <a id="upgrades"></a>Upgrades

### <a id="upgrade-broker"></a>Upgrade the Broker

The broker is upgraded in a similar manner to all BOSH releases:

* upload new version of `on-demand-service-broker-release` BOSH release to the BOSH Director
* make any necessary changes to the core broker configuration in the broker manifest
* deploy the broker

### <a id="upgrade-service-offering"></a>Upgrade Service Offering

The service offering consists of:

* service catalog
* service adapter BOSH release
* service BOSH release(s)
* service stemcell

To upgrade a service offering:

* make any changes to the service catalog in the broker manifest
* upload any new service BOSH release(s) to the BOSH Director
* make any changes to service release(s) in the broker manifest
* upload any new service stemcell to the BOSH Director
* make any changes to the service stemcell in the `service_deployment` broker manifest
* deploy the broker

Any new service instances will be created using the latest service offering.

To upgrade all existing instances you can run the [upgrade-all-service-instances errand](#upgrade-instances).

**Warning:** Until a service instance has been upgraded, `cf update-service` operations will be blocked and an error will be shown, see [updating service offering](#updating-service-offering).

### <a id="upgrade-individual-instances"></a>Upgrade an Individual Service Instance

Cloud Foundry users cannot upgrade their service instances to the latest service offering.

Until a service instance has been upgraded, Cloud Foundry users cannot set parameters, or change plan until the service instance has been upgraded by an operator:<pre class="terminal">
$ cf update-service my-redis -c '{"maxclients": 10000}'
Updating service instance my-redis as admin...
FAILED
Server error, status code: 502, error code: 10001, message: Service broker error: Service cannot be updated at this time, please try again later or contact your operator for more information.
</pre>

Operators should run the [upgrade-all-service-instances errand](#upgrade-instances) to upgrade all service instances to the latest service offering.

### <a id="upgrade-all-instances"></a>Upgrade All Service Instances

To upgrade all existing service instances after the service offering has been updated or upgraded:

<ol>
<li>Add the following instance group to your broker manifest:
    <pre>
    - name: upgrade-all-service-instances
      lifecycle: errand
      instances: 1
      jobs:
        - name: upgrade-all-service-instances
          release: ODB-RELEASE-NAME
          properties:
            canaries: NUMBER-OF-CANARIES # defaults to 0
            canary_selection_params:
              cf_org: ORG # specifying service instances to upgrade as canaries
              cf_space: SPACE # specifying service instances to upgrade as canaries
            max_in_flight: NUMBER-OF-PARALLEL-UPGRADES # defaults to 1
            polling_interval_seconds: POLLING-INTERVAL-IN-SECONDS # defaults to 60
            attempt_interval_seconds: ATTEMPT-INTERVAL-IN-SECONDS # defaults to 60
            attempt_limit: NUMBER-OF-ATTEMPTS # defaults to 5
      vm_type: VM-TYPE
      stemcell: STEMCELL
      networks: [{name: NETWORK}]
      azs: [AZ]
    </pre>
    <p>
      The errand properties allow fine-tuning of the behavior of the upgrade job.
      <ul>
      <li><code>max_in_flight</code> sets the limit for the number of upgrades
      occurring concurrently.  The number of simultaneous upgrades is limited by
      the number of available BOSH workers. See the <a
      href="https://bosh.io/jobs/director?source=github.com/cloudfoundry/bosh#p=director.workers">BOSH
      docs</a>.  Set the <code>max_in_flight</code> value to lower than this
      limit to avoid over-saturating BOSH.  </li>

      <li>
        <code>canaries</code> sets the number of canary instances to upgrade first.
        If all canary instances upgrade, the remaining instances are upgraded.
        If a canary instance fails to upgrade or the <code>attempt_limit</code>
        is reached, the upgrade fails.
        No further instances are upgraded, and the errand exits with an error;
        however, all in-flight upgrades can complete.
        Canary instances are upgraded in parallel, respecting the <code>max_in_flight</code> value.

        <p class="note"> <strong>Note:</strong> Canary instances are selected in
          a non-deterministic way using all available instances.
          If a selected instance is busy or was deleted, another instance is selected.
          If all instances are busy, the errand retries, respecting the
          <code>attempt_limit</code> and <code>attempt_interval_seconds</code>.</p>

        <li>
          (Optional) Use <code>canary_selection_params</code> to specify an org and a space
          that you want canaries to be sourced from during an upgrade.
          If <code>canaries</code> is specified, the broker upgrades that number of instances present in the org and space.
          If fewer instances are present than specified, the broker upgrades as many instances as possible
          in that org and space.
        </li>
        <p class="note"> <strong>Note:</strong> If <code>canary_selection_params</code>
           are specified and no instances exist in that org or space, no canaries will be chosen.
           If other instances exist, the broker will fail alerting you to chose different selection criteria.
         </p>
      </li>

      <li><code>polling_interval_seconds</code> controls
      the wait between checking the status of the successfully submitted BOSH upgrade job. </li>
    </ul>
    If service instances
      have in-progress BOSH operations, upgrade requests are rejected and the errand queues those instances
      for a retry:
    <ul>
      <li><code>attempt_interval_seconds</code> determines the time to wait between retrying upgrades.</li>
      <li><code>attempt_limit</code> sets the number of times these instances are retried for upgrade.</li>
    </ul>
    </p>
</li>
<li>Deploy the broker manifest.</li>
<li>Run the errand in one of two ways, depending on which version of the BOSH CLI you use:<br/>
<ul>
<li><strong>For v2 of the CLI:</strong> <code>bosh run-errand upgrade-all-service-instances</code>.</li>
<li><strong>For v1 of the CLI:</strong> <code>bosh run errand upgrade-all-service-instances</code>.</li>
</ul>
</li>
</ol>

<p class="note"><strong>Note:</strong> The <code>upgrade-all-service-instances</code> errand triggers service instance lifecycle errands configured for the broker.
   For more information, see <a href="#lifecycle-errands">Service Instance Lifecycle Errands</a> below.</p>

#### <a id="service-instances-api"></a> Service Instances API

The service instances API consists of a single HTTP endpoint that the errand will utilise to collect the required state to drive the upgrade process against your On Demand Service Broker. The API endpoint called is configurable in the upgrade-all-service-instances errand.

<p class="warning note"><strong>WARNING:</strong> The Service Instances API is an advanced feature.
Configuring a Service Instances API is not required, unless you are operating in very specific circumstances, such as running an On Demand Service Broker with no connection to Cloud Foundry.
You will be required to provide an endpoint which satisfied the API requirements detailed below, and also guarantees that all service instances which need to be upgraded are part of that response.
If you configure an endpoint which does not satisfy these criteria, some service instances may become unusable.</p>

The Service Instances API definition is at version 0.1.0 and should be considered subject to change. The API consists of a single endpoint which should handle the following request with the appropriate response:

<strong>Request:</strong>

```
HTTP Method: GET
Authentication: http/basic
Body: None
```

<strong>Response:</strong>

```
Status: 200 OK if successful, 3xx, 4xx, 5xx HTTP codes for other scenarios.
Content-Type: application/json
Body:
"[
  {
    "service_instance_id": <text/guid>,
    "plan_id": <text/guid>
  },
  ...
]"
```

The plan\_ids listed in the response need to correspond to the plan\_ids that appear in each plan's definition in the On Demand Broker deployment manifest, not any other id that may be assigned by the service controller.

When the on-demand service broker is configured to use `canary_selection_params`,
you need to change the Service Instances API accordingly.
The `canary_selection_params` property defines how to filter the canary instances from of a set of instances.
Therefore, you need to change the Service Instances API so that it responds with a filtered list of instances
when the `canary_selection_params` are passed as query parameters.

For example, if the `canary_selection_params` are configured as following:

```
canary_selection_params:
  cf_org: staging-org
  cf_space: staging-space
```

Then, the Service Instances API returns a filtered list of instances when `cf_org` and `cf_space` are passed as query parameters in the request.

<strong>Request:</strong>

```
HTTP Method: GET
Query Parameters: ?cf_org=staging-org&cf_space=staging-space
Authentication: http/basic
Body: None
```

<strong>Response:</strong>

```
Status: 200 OK if successful, 3xx, 4xx, 5xx HTTP codes for other scenarios.
Content-Type: application/json
Body:
"[
  {
    "service_instance_id": <text/guid>,
    "plan_id": <text/guid>
  },
  ...
]"
```
The instances in the filtered list are used as canary instances and upgraded before the rest.
The number of canaries taken from this list can be configured
by specifying the `canaries` property in the on-demand service broker manifest.

To configure the `upgrade-all-service-instances` errand to connect to a Service Instances API provider, update the errand's configuration in the deployment manifest to include the following additional properties:

```
- name: upgrade-all-service-instances
  lifecycle: errand
  instances: 1
  jobs:
    - name: upgrade-all-service-instances
      release: ODB-RELEASE-NAME
      properties:
        polling_interval_seconds: POLLING-INTERVAL-IN-SECONDS # defaults to 60
        attempt_interval_seconds: ATTEMPT-INTERVAL-IN-SECONDS # defaults to 60
        attempt_limit: NUMBER-OF-ATTEMPTS # defaults to 5
        service_instances_api: # optional
          url: URL-OF-SERVICE-INSTANCES-API-PROVIDER # required
          root_ca_cert: ROOT-CA-CERT # optional
          authentication:
            basic: # required - currently the only supported authentication type
              username: USERNAME
              password: PASSWORD
  vm_type: VM-TYPE
  stemcell: STEMCELL
  networks: [NETWORK]
  azs: [AZ]
```

Where: <br>
`POLLING-INTERVAL-IN-SECONDS`: The number of seconds to wait between checking the 
status of a submitted upgrade job. <br>
`ATTEMPT-INTERVAL-IN-SECONDS`: The number of seconds to wait between retrying upgrades. <br>
`NUMBER-OF-ATTEMPTS`: The maximum number of times the errand will try a set of upgrades. <br>

## <a id="security"></a>Security

### <a id="bosh-endpoints"></a>BOSH API Endpoints

The ODB accesses the following [BOSH API](https://bosh.io/docs/director-api-v1.html) endpoints during the service instance lifecycle:

| API endpoint                                                     | Examples of usage in the ODB                                                                                                                                 |
|:-----------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `POST /deployments`                                              | create, or update a service instance                                                                                                                         |
| `POST /deployments/DEPLOYMENT-NAME/errands/ERRAND-NAME/runs` | register, or de-register the on-demand broker with the Cloud Controller, run smoke tests                                                                     |
| `GET /deployments/DEPLOYMENT-NAME`                             | passed as argument to the service adapter for `generate-manifest` and `create-binding`                                                                       |
| `GET /deployments/DEPLOYMENT-NAME/vms?format=full`             | passed as argument to the service adapter for `create-binding`                                                                                               |
| `DELETE /deployments/DEPLOYMENT-NAME`                          | delete a service instance                                                                                                                                    |
| `GET /tasks/TASK-ID/output?type=result`                        | check a task was successful (i.e. the exit code was zero), get list of VMs                                                                                   |
| `GET /tasks/TASK-ID`                                           | poll the BOSH Director until a task finishes, e.g. create, update, or delete a deployment                                                                    |
| `GET /tasks?deployment=DEPLOYMENT-NAME`                        | determine the last operation status and message for a service instance, e.g. 'create in progress' - used when creating, updating, deleting service instances |

### <a id="bosh-uaa"></a>BOSH UAA Permissions

The actions that the ODB needs to be able to perform are:

Modify:

- `bosh deploy`
- `bosh delete deployment`
- `bosh run errand`

Read only:

- `bosh deployments`
- `bosh vms`
- `bosh tasks`

The minimum UAA authority required by the BOSH Director to perform these actions is `bosh.teams.TEAM.admin`. Note: a team admin cannot view or update the Director's cloud config, nor upload releases or stemcells.

For more details on how to set up and use BOSH teams, see [Director teams and permissions configuration](https://bosh.io/docs/director-users-uaa-perms.html).

#### Unused BOSH permissions
The team admin authority also allows the following actions, which currently are not used by the ODB:

- `bosh start/stop/recreate`
- `bosh cck`
- `bosh logs`
- `bosh releases`
- `bosh stemcells`

### <a id="ipsec"></a>PCF IPsec Add-On

The ODB has been tested with the [PCF IPsec Add-On](https://docs.pivotal.io/addon-ipsec/installing.html), and it appears to work. Note that we excluded the BOSH Director itself from IPsec ranges, as the BOSH add-on cannot be applied to BOSH itself.

### <a id="cf-endpoints"></a>CF API Endpoints

The ODB accesses the following [CF API](https://apidocs.cloudfoundry.org/268/) endpoints during the service instance lifecycle:

| API endpoint                                                         | Examples of usage in the ODB                                                                           |
|:---------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------|
| `GET /v2/info`                                                       | identify CF API version, to determine feature compatibility & availability                             |
| `GET /v2/services`                                                   | list all services, to find our own service based on defined unique ID rather than GUID                 |
| `GET /v2/services/SERVICE-GUID/service_plans`                      | find registered service plans for ODB service e.g. for calculating plan quota usage                    |
| `GET /v2/service_brokers`                                            | find service broker metadata by name during broker deregistration                                      |
| `DELETE /v2/service_brokers/SERVICE-BROKER-GUID`                   | delete ODB service broker during broker deregister errand                                              |
| `GET /v2/service_plans/SERVICE-PLAN-GUID`                          | identify service plan when upgrading an instance to trigger any lifecycle errands                      |
| `PUT /v2/service_plans/SERVICE-PLAN-GUID`                          | disable service access prior to service deletion                                                       |
| `GET /v2/service_plans/SERVICE-PLAN-GUID/service_instances`        | find service instances for given plan when determining global quota and running startup checks         |
| `GET /v2/service_instances/SERVICE-INSTANCE-GUID`                  | determine service instance state to check an operation is not in progress before triggering an upgrade |
| `DELETE /v2/service_instances/SERVICE-INSTANCE-GUID`               | deleting a service instance during delete all service instances errand                                 |
| `GET /v2/service_instances/SERVICE-INSTANCE-GUID/service_bindings` | finding bindings for given service instance during delete all service instances errand                 |
| `GET /v2/service_instances/SERVICE-INSTANCE-GUID/service_keys`     | finding service keys for given service instance during delete all service instances errand             |
| `DELETE /v2/apps/APP-GUID/service_bindings/SERVICE-BINDING-GUID` | unbinding a service instance during delete all service instances errand                                |
| `DELETE /v2/service_keys/SERVICE-KEY-GUID`                         | deleting a service key during delete all service instances errand                                      |

### <a id="cf-uaa"></a>CF UAA Permissions

The actions that the ODB needs to be able to perform are:

Modify:

- `cf enable-service-access`
- `cf disable-service-access`
- `cf create-service-broker`
- `cf delete-service-broker`
- `cf delete-service`
- `cf unbind-service`
- `cf delete-service-key`

Read only:

- `cf api`
- `cf marketplace`
- `cf service-brokers`
- `cf services`
- `cf service`
- `cf app`
- `cf service-keys`

The minimum UAA authority required by CF to perform these actions is `cloud_controller.admin`. Admin is required as many operations are required to perform against all of the on demand service instances across a foundation, regardless of org and space.

#### Unused CF permissions
The Cloud Controller admin authority also allows the following actions, which currently are not used by the ODB:

- `cf push`
- `cf delete`
- `cf start`
- `cf restart`
- `cf restage`
- `cf stop`
- `cf create-service-key`
- `cf create-user-provided-service`
- `cf update-user-provided-service`
- `cf run-task`
- `cf logs`
- `cf ssh`
- `cf scale`
- `cf events`
- Route and domain management
- Space management
- Org management
- CLI plugin management

## <a id="troubleshooting"></a>Troubleshooting

### <a id="admin-instances"></a>Administer Service Instances

We recommend using the [BOSH CLI gem](https://bosh.io/docs/bosh-cli.html) for administering the deployments created by ODB; for example for checking VMs, ssh, viewing logs.

Pivotal does **not** recommend using the BOSH CLI to update or delete ODB service deployments as it might accidentally trigger a race condition with Cloud Controller-induced updates/deletes or result in ODB overriding your [snowflake](http://martinfowler.com/bliki/SnowflakeServer.html) changes at the next deploy. All updates to the service instances must be done using the [errand to upgrade all service instances](#upgrade-instances).

### <a id="logs"></a>Logs

The on-demand broker writes logs to a log file, and to syslog.

The broker log contains error messages and non-zero exit codes returned by the service adapter, as well as the stdout and stderr streams of the adapter.

The log file is located at `/var/vcap/sys/log/broker/broker.log`. In syslog, logging is written with the tag `on-demand-service-broker`, under the facility `user`, with priority `info`.

If you want to forward syslog to a syslog aggregator, we recommend co-locating [syslog release](https://github.com/cloudfoundry/syslog-release) with the broker.

The ODB generates a UUID for each request and prefixes all the logs for that request, e.g.

<pre class="terminal">
[on-demand-service-broker] [4d63080d-e038-45a3-85f9-93910f6b40b1] 2016/09/05 16:43:26.123456 a valid UAA token was found in cache, will not obtain a new one
</pre>

NB: The ODB's negroni server and start up logs are not prefixed with a request ID.

All ODB's logs have a UTC timestamp.

#### <a id="errands-syslog"></a> Syslog forwarding for errands logs

If you want to forward your errand logs to a syslog aggregator, we recommend
co-locating [syslog release](https://github.com/cloudfoundry/syslog-release)
with the errand job. The manifest will look similar to the following:

```
- name: delete-all-service-instances-and-deregister-broker
  lifecycle: errand
  ...
  jobs:
  - name: delete-all-service-instances-and-deregister-broker
    release: on-demand-service-broker
    ...
  - name: syslog_forwarder
    release: syslog
    properties:
      syslog:
        address: ((syslog.address))
        port: ((syslog.port))
        transport: udp
        forward_files: false
        custom_rule: |
          module(load="imfile" mode="polling")
          input(type="imfile"
                File="/var/vcap/sys/log/delete-all-service-instances-and-deregister-broker/errand.stdout.log"
                Tag="delete-all-service-instances-and-deregister-broker")
          input(type="imfile"
                File="/var/vcap/sys/log/delete-all-service-instances-and-deregister-broker/errand.stderr.log"
                Tag="delete-all-service-instances-and-deregister-broker")
```

<p class="note"><strong>Note</strong>: The errand is configured to redirect stdout and stderr to
<code>/var/vcap/sys/log/ERRAND_NAME/errand.stdout.log</code> and
<code>/var/vcap/sys/log/ERRAND_NAME/errand.stderr.log</code>. When configuring your errand,
be careful to match the actual log file paths in the <code>custom_rule</code> section.</p>

### <a id="secure-binding-troubleshooting"></a> Secure Binding Credentials

If you have [configured secure binding credentials](#secure-binding), the broker
will store credentials on CredHub. You can see and consume these credentials by
using the [CredHub CLI](https://github.com/cloudfoundry-incubator/credhub-cli).

<p class="note"><strong>Note</strong>: Usually, CredHub is not accessible from
outside the Cloud Foundry network. Use the CredHub CLI from within the internal
network, or connect using an appropriate tunnel.</p>

In failure scenarios, like when CredHub is down or when the CredHub client
credentials are wrong, the broker will log to the file at
`/var/vcap/sys/log/broker/broker.log` where the root cause will be generally
given. See [Troubleshooting > Logs](#logs) for details.

**Common causes of errors**

* CredHub down / Wrong CredHub URL / Cannot access URL
* Wrong credentials to access CredHub
* Problem with CA certs for CredHub or UAA
* Binding credentials in an exotic format (the broker only accepts string and
string map credentials)


### <a id="metrics"></a>Metrics

If you have [configured broker metrics](#broker-metrics), the broker will emit metrics to the CF firehose. You can, for example, consume these metrics by using the [CF CLI firehose plugin](https://github.com/cloudfoundry/firehose-plugin).

<p class="note"><strong>Note</strong>: The broker must be <a href="#register">registered with a Cloud Foundry</a> in order for metrics to be successfully emitted</p>

#### Service-level Metrics
The broker will emit a metric indicating the total number of instances across all plans. In addition, if there is a global quota set for the service, a metric showing how much of that quota is remaining will be emitted. Service-level metrics use the format shown below.

```
origin:"BROKER-DEPLOYMENT-NAME" eventType:ValueMetric timestamp:TIMESTAMP deployment:"BROKER-DEPLOYMENT-NAME" job:"broker" index:"BOSH-JOB-INDEX" ip:"IP" valueMetric:<name:"/on-demand-broker/SERVICE-OFFERING-NAME/total_instances" value:INSTANCE-COUNT unit:"count" >
origin:"BROKER-DEPLOYMENT-NAME" eventType:ValueMetric timestamp:TIMESTAMP deployment:"BROKER-DEPLOYMENT-NAME" job:"broker" index:"BOSH-JOB-INDEX>" ip:"IP" valueMetric:<name:"/on-demand-broker/SERVICE-OFFERING-NAME/quota_remaining" value:QUOTA-REMAINING unit:"count" >
```

#### Plan-level Metrics
For each service plan, the metrics will report the total number of instances for that plan. If there is a quota set for the plan, the metrics will also report how much of that quota is remaining. Plan-level metrics are emitted in the following format.

```
origin:"BROKER-DEPLOYMENT-NAME" eventType:ValueMetric timestamp:TIMESTAMP deployment:"BROKER-DEPLOYMENT-NAME" job:"broker" index:"BOSH-JOB-INDEX" ip:"IP" valueMetric:<name:"/on-demand-broker/SERVICE-OFFERING-NAME/PLAN-NAME/total_instances" value:INSTANCE-COUNT unit:"count" >
origin:"BROKER-DEPLOYMENT-NAME" eventType:ValueMetric timestamp:TIMESTAMP deployment:"BROKER-DEPLOYMENT-NAME" job:"broker" index:"BOSH-JOB-INDEX" ip:"IP" valueMetric:<name:"/on-demand-broker/SERVICE-OFFERING-NAME/PLAN-NAME/quota_remaining" value:QUOTA-REMAINING unit:"count" >
```

If `quota_remaining` is `0` then you need to increase your plan quota in the BOSH manifest.

### <a id="identifying-deployments"></a>Identify Deployments in BOSH

There is a one to one mapping between the service instance id from CF and the deployment name in BOSH. The convention is the BOSH deployment name would be the service instance id prepended by `service-instance_`. To identify the BOSH deployment for a service instance you can.

1. Determine the GUID of the service
   <pre class="terminal">
    $ cf service --guid SERVICE-NAME
   </pre>

1. Identify deployment in `bosh deployments` by looking for `service-instance_GUID`

1. Get current tasks for the deployment by using
   <pre class="terminal">
    $ bosh tasks --deployment service-instance_GUID
   </pre>

### <a id="id-tasks"></a>Identify Tasks in BOSH

Most operations on the on demand service broker API are implemented by launching BOSH tasks. If an operation fails, it may be useful to investigate the corresponding BOSH task. To do this:

1. Determine the ID of the service for which an operation failed. You can do this using the Cloud Foundry CLI:
   <pre class="terminal">
    $ cf service --guid SERVICE-NAME
   </pre>

1. SSH on to the service broker VM:
   <pre class="terminal">
    $ bosh deployment PATH-TO-BROKER-MANIFEST
    $ bosh ssh
   </pre>

1. In the broker log, look for lines relating to the service, identified by the service ID. Lines recording the starting and finishing of BOSH tasks will also have the BOSH task ID:

    ```
    on-demand-service-broker: [on-demand-service-broker] [4d63080d-e038-45a3-85f9-93910f6b40b1] 2016/04/13 09:01:50.793965 Bosh task id for Create instance 30d4a67f-d220-4d06-9989-58a976b86b35 was 11470
    on-demand-service-broker: [on-demand-service-broker] [4d63080d-e038-45a3-85f9-93910f6b40b1] 2016/04/13 09:06:55.793976 task 11470 success creating deployment for instance 30d4a67f-d220-4d06-9989-58a976b86b35: create deployment

    on-demand-service-broker: [on-demand-service-broker] [8bf5c9f6-7acd-4ab4-9214-363a6f6bef79] 2016/04/13 09:16:20.795035 Bosh task id for Update instance 30d4a67f-d220-4d06-9989-58a976b86b35 was 11473
    on-demand-service-broker: [on-demand-service-broker] [8bf5c9f6-7acd-4ab4-9214-363a6f6bef79] 2016/04/13 09:17:20.795181 task 11473 success updating deployment for instance 30d4a67f-d220-4d06-9989-58a976b86b35: create deployment

    on-demand-service-broker: [on-demand-service-broker] [af6fab15-c95e-438b-aa6b-bc4329d4154f] 2016/04/13 09:17:52.803824 Bosh task id for Delete instance 30d4a67f-d220-4d06-9989-58a976b86b35 was 11474
    on-demand-service-broker: [on-demand-service-broker] [af6fab15-c95e-438b-aa6b-bc4329d4154f] 2016/04/13 09:19:56.803938 task 11474 success deleting deployment for instance 30d4a67f-d220-4d06-9989-58a976b86b35: delete deployment service-instance_30d4a67f-d220-4d06-9989-58a976b86b35
    ```

1. Use the task ID to obtain the task log from BOSH (adding flags such as `--debug` or `--cpi` as necessary):
   <pre class="terminal">
    $ bosh task TASK-ID
   </pre>

### <a id="id-uaa-issues"></a>Identify Issues When Connecting to BOSH or UAA

The ODB interacts with the BOSH Director to provision and deprovision instances, and is authenticated via the Director's UAA. See [Core Broker Configuration](#core-broker-configuration) for an example configuration.

If BOSH and/or UAA are wrongly configured in the broker's manifest, then meaningful error messages will be displayed in the broker's log, indicating whether the issue is caused by an unreachable destination or bad credentials.

For example

```
on-demand-service-broker: [on-demand-service-broker] [575afbc1-b541-481d-9cde-b3d3e67e87bf] 2016/05/18 15:56:40.100579 Error authenticating (401): {"error":"unauthorized","error_description":"Bad credentials"}, ensure that properties.BROKER-JOB.bosh.authentication.uaa is correct and try again.
```

### <a id="listing-instances"></a>List Service Instances

The ODB persists the list of ODB-deployed service instances and provides an endpoint to retrieve them. This endpoint requires basic authentication.

During disaster recovery this endpoint could be used to assess the situation.

**Request**

`GET http://USERNAME:PASSWORD@ON-DEMAND-BROKER-IP:8080/mgmt/service_instances`

**Response**

200 OK

Example JSON body:

  ```json
  [
    {
      "instance_id": "4d19462c-33cf-11e6-91cc-685b3585cc4e",
      "plan_id": "60476620-33cf-11e6-a841-685b3585cc4e",
      "bosh_deployment_name": "service-instance_4d19462c-33cf-11e6-91cc-685b3585cc4e"
    },
    {
      "instance_id": "57014734-33cf-11e6-ba8d-685b3585cc4e",
      "plan_id": "60476620-33cf-11e6-a841-685b3585cc4e",
      "bosh_deployment_name": "service-instance_57014734-33cf-11e6-ba8d-685b3585cc4e"
    }
  ]
  ```

### <a id="listing-orphans"></a>List Orphan Deployments

The On-Demand Broker provides an endpoint that compares the list of service instance
deployments against the service instances registered in Cloud Foundry. When called,
the endpoint will return a list of orphaned deployments, if any are present.

This endpoint is exercised in the [`orphan-deployments`](#orphan-deployments) errand. To call this endpoint
without  running the errand, use curl

**Request**
`GET http://USERNAME:PASSWORD@ON-DEMAND-BROKER-IP:8080/mgmt/orphan_deployments`

**Response**

200 OK

Example JSON body:

```json
[
  {
	  "deployment_name": "service-instance_d482abd3-8051-48d2-8067-9ccdf02327f3"
  }
]
```
